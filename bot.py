import os
import json
from dotenv import load_dotenv
from openai import OpenAI
from pymilvus import Collection, connections
from embedding import to_vector
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory

# === åˆå§‹åŒ– ===
load_dotenv()
model_name = "gpt-4o-mini"
SIMILARITY_THRESHOLD = float(os.getenv("SIMILARITY_THRESHOLD"))
chat_model = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"), model_name=model_name)
llm_api = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === ç‹€æ…‹ç®¡ç† ===
def load_user_context(user_id: str) -> dict:
    os.makedirs("sessions", exist_ok=True)
    os.makedirs("profiles", exist_ok=True)
    summary_path = f"sessions/{user_id}_summary.json"
    profile_path = f"profiles/{user_id}.json"

    if not os.path.exists(summary_path):
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump({"summary": ""}, f)

    if not os.path.exists(profile_path):
        with open(profile_path, "w", encoding="utf-8") as f:
            json.dump({"age": None, "personality": "æº«å’Œ"}, f)

    with open(summary_path, "r", encoding="utf-8") as f:
        summary = json.load(f).get("summary", "")
    with open(profile_path, "r", encoding="utf-8") as f:
        profile = json.load(f)

    return {"summary": summary, "profile": profile}

# === Tool 1: RAG æŸ¥è©¢ ===
@tool
def search_milvus(query: str) -> str:
    """åœ¨ Milvus è³‡æ–™åº«ä¸­æŸ¥è©¢ COPD è¡›æ•™å•ç­”ï¼Œå›å‚³ç›¸ä¼¼å•é¡Œèˆ‡ç­”æ¡ˆ"""
    try:
        connections.connect(alias="default", uri="http://localhost:19530")
        collection = Collection("copd_qa")
        collection.load()
        user_vec = to_vector(query)
        if not isinstance(user_vec, list):
            user_vec = user_vec.tolist() if hasattr(user_vec, 'tolist') else list(user_vec)
        results = collection.search(
            data=[user_vec],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"nprobe": 10}},
            limit=5,
            output_fields=["question", "answer", "category"],
        )
        connections.disconnect(alias="default")

        chunks = []
        for hit in results[0]:
            if hit.score >= SIMILARITY_THRESHOLD:
                q = hit.entity.get("question")
                a = hit.entity.get("answer")
                cat = hit.entity.get("category")
                chunks.append(f"[{cat}] (ç›¸ä¼¼åº¦: {hit.score:.3f})\nQ: {q}\nA: {a}")

        return "\n\n".join(chunks) if chunks else "[æŸ¥ç„¡é«˜ç›¸ä¼¼åº¦çµæœ]"
    except Exception as e:
        return f"[Milvus éŒ¯èª¤] {e}"

# === Tool 2: å°è©±æ‘˜è¦ ===
@tool
def summarize_conversation(user_id: str) -> str:
    """æ‘˜è¦æ•´æ®µå°è©±ç´€éŒ„ï¼Œä¸¦æ›´æ–°ä½¿ç”¨è€…çš„æ‘˜è¦æª”æ¡ˆ"""
    session_path = f"sessions/{user_id}.json"
    summary_path = f"sessions/{user_id}_summary.json"
    if not os.path.exists(session_path):
        return "ç›®å‰ç„¡å¯ä¾›æ‘˜è¦çš„å°è©±ç´€éŒ„ã€‚"

    with open(session_path, "r", encoding="utf-8") as f:
        history = json.load(f)
    text = "".join([f"ç¬¬{i+1}è¼ª:\né•·è¼©: {h['input']}\né‡‘å­«: {h['output']}\n\n" for i, h in enumerate(history)])
    prompt = f"""
è«‹ç‚ºä»¥ä¸‹å°è©±ç”Ÿæˆæ‘˜è¦ï¼Œæ¶µè“‹å¥åº·å•é¡Œã€å»ºè­°é‡é»ã€æƒ…ç·’æ°›åœï¼š\n{text}è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œ100-150å­—ã€‚
"""

    try:
        res = llm_api.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æ‘˜è¦åŠ©æ‰‹"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        summary = res.choices[0].message.content.strip()
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump({"summary": summary}, f, ensure_ascii=False, indent=2)
        with open(session_path, "w", encoding="utf-8") as f:
            json.dump([], f)
        return summary
    except Exception as e:
        return f"[æ‘˜è¦éŒ¯èª¤] {e}"

# === å»ºç«‹èŠå¤© Agent ===
def build_agent(user_id: str):
    context = load_user_context(user_id)
    profile = context["profile"]
    summary = context["summary"]

    profile_txt = f"ä½¿ç”¨è€…å¹´é½¡ï¼š{profile.get('age', 'æœªçŸ¥')}ï¼Œå€‹æ€§ï¼š{profile.get('personality', 'æº«å’Œ')}\n"
    summary_txt = f"\n\nğŸ“Œ æ­·å²æ‘˜è¦ï¼š\n{summary}" if summary else ""

    system_msg = f"""
ä½ æ˜¯ä¸€ä½æœƒèªªå°ç£é–©å—èªçš„å¥åº·é™ªä¼´æ©Ÿå™¨äººã€‚
{profile_txt}
ä½ å¯ä»¥ä½¿ç”¨ search_milvus æŸ¥è©¢å¥åº·çŸ¥è­˜åº«ï¼Œæˆ–ä½¿ç”¨ summarize_conversation ä¾†ç¸½çµæœ€è¿‘çš„å°è©±ã€‚
è«‹æ ¹æ“šéœ€è¦æ±ºå®šæ˜¯å¦ä½¿ç”¨é€™äº›å·¥å…·ã€‚è«‹ä»¥è¦ªåˆ‡å°èªé€²è¡Œå°è©±ã€‚{summary_txt}
""".strip()

    tools = [search_milvus, summarize_conversation]
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    agent = initialize_agent(
        tools=tools,
        llm=chat_model,
        agent=AgentType.OPENAI_FUNCTIONS,
        verbose=True,
        memory=memory,
        agent_kwargs={"system_message": system_msg}
    )
    return agent

# === é›¢ç·šå‰è‡ªå‹•æ‘˜è¦ ===
def auto_save_and_summary(user_id: str):
    print("ğŸ“ è‡ªå‹•å„²å­˜ä¸¦é€²è¡Œå°è©±æ‘˜è¦ä¸­...")
    print(summarize_conversation.invoke({"user_id": user_id}))
