import os
from time import time

from dotenv import load_dotenv
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_ollama import OllamaLLM
from pymilvus import Collection, connections

from embedding import to_vector

# è¼‰å…¥ .env
load_dotenv()

# ä¸­æ–‡æ‘˜è¦ prompt
SUMMARY_PROMPT = """ä½ æ˜¯å°è©±æ‘˜è¦åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å°‡ä»¥ä¸‹å°è©±æ•´ç†ç‚ºç°¡æ½”æ‘˜è¦ã€‚
ğŸ‘“ å…ˆå‰æ‘˜è¦ï¼š
{summary}

ğŸ’¬ æœ¬è¼ªå°è©±ï¼š
{new_lines}

ğŸ“ è«‹ç”¢ç”Ÿä¸€æ®µæ›´æ–°å¾Œçš„æ‘˜è¦ï¼š
"""


# System äººè¨­
SYSTEM_PROMPT = os.getenv("SYS_PROMPT").replace("\\n", "\n")

# æ˜ç¢ºæè¿°çŸ¥è­˜åº«ç¯„åœ
KNOWLEDGE_BASE_SCOPE = os.getenv("KNOWLEDGE_BASE_SCOPE").replace("\\n", "\n")


class HealthChatAgent:
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.llm = self._init_llm()
        self.memory = self._init_memory()

    def _init_llm(self):
        return OllamaLLM(model="adsfaaron/taide-lx-7b-chat:q5")

    def _init_memory(self):
        return ConversationSummaryMemory(
            llm=self.llm,
            memory_key="chat_history",
            prompt=ChatPromptTemplate.from_template(SUMMARY_PROMPT),
        )

    def search_milvus(self, user_text):
        try:
            connections.connect(alias="default", uri="http://localhost:19530")
            collection = Collection("copd_qa")
            collection.load()
            user_vec = to_vector(user_text)
            results = collection.search(
                data=[user_vec],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=3,
                output_fields=["question", "answer", "category"],
            )
            connections.disconnect(alias="default")
            threshold = float(os.getenv("SIMILARITY_THRESHOLD"))
            relevant_chunks = []
            print("\nğŸ” å‰ 3 ç­†ç›¸ä¼¼ QAï¼ˆå«ç›¸ä¼¼åº¦ï¼‰")
            for i, hit in enumerate(results[0]):
                score = hit.score
                q = hit.entity.get("question")
                a = hit.entity.get("answer")
                cat = hit.entity.get("category")
                print(f"Top {i+1} | ç›¸ä¼¼åº¦: {score:.4f}\n[{cat}] Q: {q}\nA: {a}\n")
                if score >= threshold:
                    relevant_chunks.append(f"[{cat}]\nQ: {q}\nA: {a}")
            return relevant_chunks
        except Exception as e:
            print(f"[Milvus éŒ¯èª¤] {e}")
            return []

    def intent_detect(self, user_input):
        light_llm = OllamaLLM(model="qwen:1.8b-chat")  # å°ä¸­æ–‡æ¨¡å‹
        prompt = (
            f"{KNOWLEDGE_BASE_SCOPE}\n\n"
            f"ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…çš„å•é¡Œï¼Œè«‹åˆ¤æ–·æ˜¯å¦éœ€è¦æŸ¥è©¢çŸ¥è­˜åº«æ‰èƒ½æ­£ç¢ºå›ç­”ï¼š\n"
            f"ã€Œ{user_input}ã€\n\n"
            f"è«‹åªå›ç­” yes æˆ– noï¼Œä¸è¦åŠ å…¶ä»–æ–‡å­—ã€‚"
        )
        resp = light_llm.invoke([HumanMessage(prompt)])
        print("ğŸ¤– å°æ¨¡å‹æ„åœ–åˆ¤æ–·çµæœï¼š", resp)
        return "yes" in resp.lower()

    def build_prompt_by_template(self, user_input, context=None, summary=None):
        sys_prompt = SYSTEM_PROMPT
        base_template = os.getenv("BASE_PROMPT_TEMPLATE")

        context_block = f"ğŸ“š ä»¥ä¸‹ç‚ºåƒè€ƒè³‡æ–™ï¼š\n{context.strip()}" if context else ""
        summary_block = f"ğŸ§  å°è©±æ‘˜è¦ï¼š\n{summary.strip()}" if summary else ""

        full_prompt = base_template.format(
            sys_prompt=sys_prompt,
            context_block=context_block,
            summary_block=summary_block,
            user_input=user_input.strip(),
        )

        return [SystemMessage(content=sys_prompt), HumanMessage(content=full_prompt)]

    def chat(self, user_input):
        # === 1. æ˜¯å¦éœ€æŸ¥è©¢çŸ¥è­˜åº« ===
        need_rag = self.intent_detect(user_input)
        print("ğŸ¤– å°æ¨¡å‹æ„åœ–åˆ¤æ–·çµæœï¼š", need_rag)
        chunks = self.search_milvus(user_input) if need_rag else []
        context = "\n\n".join(chunks) if chunks else None

        # === 2. è¼‰å…¥æ‘˜è¦è¨˜æ†¶ï¼ˆç¬¬äºŒè¼ªèµ·æ‰æœƒæœ‰ï¼‰ ===
        history = self.memory.load_memory_variables({})["chat_history"]
        summary = history if history.strip() else None

        # === 3. çµ„è£ prompt ===
        chat_messages = self.build_prompt_by_template(
            user_input=user_input, context=context, summary=summary
        )

        # === 4. å–å¾—å›è¦†ä¸¦è¨˜æ†¶ ===
        response = self.llm.invoke(chat_messages)
        self.memory.save_context({"input": user_input}, {"output": response})

        print("\nğŸ§  ä½¿ç”¨è€…æ‘˜è¦è¨˜æ†¶ï¼š")
        print(self.memory.load_memory_variables({})["chat_history"])

        return response


def main():
    print("ğŸ‘¤ å¤šä½¿ç”¨è€…å¥åº·å°è©±æ¸¬è©¦æ¨¡å¼")
    user_id = input("è«‹è¼¸å…¥æ¸¬è©¦ç”¨çš„ user_idï¼š").strip()
    agent = HealthChatAgent(user_id)
    print("\nâœ… å°è©±å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' çµæŸã€‚\n")
    while True:
        user_text = input("ğŸ§“ é•·è¼©ï¼š")
        if user_text.lower() in ["exit", "quit"]:
            break
        start_time = time()
        reply = agent.chat(user_text)
        print("ğŸ‘§ é‡‘å­«ï¼š", reply)
        print(f"â±ï¸ åŸ·è¡Œæ™‚é–“ï¼š{time() - start_time:.2f} ç§’\n")


if __name__ == "__main__":
    main()
