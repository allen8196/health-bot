import os
from time import time
from dotenv import load_dotenv
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_ollama import OllamaLLM
from pymilvus import Collection, connections
from embedding import to_vector

# è¼‰å…¥ .env
load_dotenv()

# ä¸­æ–‡æ‘˜è¦ prompt
SUMMARY_PROMPT = """è«‹å°‡ä»¥ä¸‹ä½¿ç”¨è€…èˆ‡åŠ©ç†çš„å°è©±æ‘˜è¦ç‚ºä¸€æ®µç°¡æ½”æè¿°ï¼Œæ‘˜è¦è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

{summary}
å°è©±å…§å®¹ï¼š
{new_lines}
æ‘˜è¦ï¼š"""

# System äººè¨­
SYSTEM_PROMPT = os.getenv("SYS_PROMPT").replace("\\n", "\n")

# æ˜ç¢ºæè¿°çŸ¥è­˜åº«ç¯„åœ
KNOWLEDGE_BASE_SCOPE = (
    "çŸ¥è­˜åº«åƒ…åŒ…å«èˆ‡ COPDï¼ˆæ…¢æ€§é˜»å¡æ€§è‚ºç—…ï¼‰èˆ‡å‘¼å¸é“ä¿å¥æœ‰é—œçš„å•é¡Œèˆ‡ç­”æ¡ˆï¼Œ"
    "ç¯„åœæ¶µè“‹ï¼šç–¾ç—…ç—‡ç‹€ã€æ—¥å¸¸ç…§è­·ã€é‹å‹•æŒ‡å°ã€é£²é£Ÿå»ºè­°ã€å‘¼å¸è¨“ç·´ç­‰ã€‚"
    "ä¸åŒ…å«è—¥ç‰©æ©Ÿè½‰ã€å¤–ç§‘æ²»ç™‚ã€éå‘¼å¸ç›¸é—œç–¾ç—…ï¼ˆå¦‚ç³–å°¿ç—…ã€é«˜è¡€å£“ï¼‰ç­‰å…¶ä»–é†«ç™‚ä¸»é¡Œã€‚"
)

class HealthChatAgent:
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.llm = self._init_llm()
        self.memory = self._init_memory()
        self.dialog_count = 0

    def _init_llm(self):
        return OllamaLLM(model="adsfaaron/taide-lx-7b-chat:q5")

    def _init_memory(self):
        return ConversationSummaryMemory(
            llm=self.llm,
            memory_key="chat_history",
            prompt=ChatPromptTemplate.from_template(SUMMARY_PROMPT),
        )

    def search_milvus(self, user_text):
        try:
            connections.connect(alias="default", uri="http://localhost:19530")
            collection = Collection("copd_qa")
            collection.load()
            user_vec = to_vector(user_text)
            results = collection.search(
                data=[user_vec],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=3,
                output_fields=["question", "answer", "category"],
            )
            connections.disconnect(alias="default")
            threshold = float(os.getenv("SIMILARITY_THRESHOLD"))
            relevant_chunks = []
            print("\nğŸ” å‰ 3 ç­†ç›¸ä¼¼ QAï¼ˆå«ç›¸ä¼¼åº¦ï¼‰")
            for i, hit in enumerate(results[0]):
                score = hit.score
                q = hit.entity.get("question")
                a = hit.entity.get("answer")
                cat = hit.entity.get("category")
                print(f"Top {i+1} | ç›¸ä¼¼åº¦: {score:.4f}\n[{cat}] Q: {q}\nA: {a}\n")
                if score >= threshold:
                    relevant_chunks.append(f"[{cat}]\nQ: {q}\nA: {a}")
            return relevant_chunks
        except Exception as e:
            print(f"[Milvus éŒ¯èª¤] {e}")
            return []

    def intent_detect(self, user_input):
        """
        åˆ©ç”¨ LLM æ ¹æ“šçŸ¥è­˜åº«ç¯„åœåˆ¤æ–·æ˜¯å¦éœ€è¦æŸ¥è©¢çŸ¥è­˜åº«ï¼ˆRAGï¼‰ã€‚
        å›å‚³ True/False
        """
        judge_prompt = (
            f"{KNOWLEDGE_BASE_SCOPE}\n\n"
            f"è«‹åˆ¤æ–·ä¸‹åˆ—ç”¨æˆ¶ç™¼å•ï¼Œæ˜¯å¦ã€å¿…é ˆæŸ¥è©¢ä¸Šè¿°çŸ¥è­˜åº«æ‰èƒ½æä¾›æ­£ç¢ºç­”æ¡ˆã€ï¼Ÿ"
            f"è‹¥éœ€è¦è«‹åªå›ç­”yesï¼Œä¸éœ€è¦è«‹åªå›ç­”noã€‚\n\n"
            f"ç”¨æˆ¶ç™¼å•ï¼š{user_input}"
        )
        resp = self.llm.invoke([SystemMessage("ä½ æ˜¯çŸ¥è­˜åº«æŸ¥è©¢æ„åœ–åˆ¤æ–·å“¡"), HumanMessage(judge_prompt)])
        return "yes" in resp.lower()

    def build_prompt(self, user_input, context="", history="", step_by_step=True):
        chat_messages = [SystemMessage(content=SYSTEM_PROMPT)]
        if context:
            chat_messages.append(HumanMessage(content=f"ä»¥ä¸‹æ˜¯ä½ å¯ä»¥åƒè€ƒçš„å¥åº·è³‡æ–™ï¼š\n{context}"))
        if history:
            chat_messages.append(HumanMessage(content=f"éå»çš„å°è©±æ‘˜è¦å¦‚ä¸‹ï¼š\n{history}"))
        if step_by_step:
            user_input = f"è«‹ç”¨æ­¥é©Ÿèªªæ˜æ–¹å¼å›ç­”ã€‚{user_input}"
        chat_messages.append(HumanMessage(content=user_input))
        return chat_messages

    def chat(self, user_input):
        # === 1. é€²è¡Œæ„åœ–åˆ¤æ–·ï¼ˆè®“LLMæ ¹æ“šçŸ¥è­˜åº«ç¯„åœåˆ¤æ–·è¦ä¸è¦æŸ¥RAGï¼‰===
        need_rag = self.intent_detect(user_input)
        # === 2. æª¢ç´¢RAG ===
        chunks = self.search_milvus(user_input) if need_rag else []
        # === 3. è®€å–éå»æ‘˜è¦ ===
        history = self.memory.load_memory_variables({})["chat_history"]
        # === 4. çµ„è£ prompt ä¸¦å‘¼å« LLM ===
        chat_messages = self.build_prompt(user_input, context="\n\n".join(chunks), history=history)
        response = self.llm.invoke(chat_messages)
        # === 5. å„²å­˜è¨˜æ†¶ï¼Œè¼ªæ¬¡ç®¡ç† ===
        self.memory.save_context({"input": user_input}, {"output": response})
        self.dialog_count += 1
        if self.dialog_count % 3 == 0:
            self.memory.clear()
            print("ğŸ“ å·²å°è©±ä¸‰è¼ªï¼Œè‡ªå‹•æ‘˜è¦ä¸¦é‡ç½® historyï¼")
        print("\nğŸ§  ä½¿ç”¨è€…æ‘˜è¦è¨˜æ†¶ï¼š")
        print(self.memory.load_memory_variables({})["chat_history"])
        return response

def main():
    print("ğŸ‘¤ å¤šä½¿ç”¨è€…å¥åº·å°è©±æ¸¬è©¦æ¨¡å¼")
    user_id = input("è«‹è¼¸å…¥æ¸¬è©¦ç”¨çš„ user_idï¼š").strip()
    agent = HealthChatAgent(user_id)
    print("\nâœ… å°è©±å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' çµæŸã€‚\n")
    while True:
        user_text = input("ğŸ§“ é•·è¼©ï¼š")
        if user_text.lower() in ["exit", "quit"]:
            break
        start_time = time()
        reply = agent.chat(user_text)
        print("ğŸ‘§ é‡‘å­«ï¼š", reply)
        print(f"â±ï¸ åŸ·è¡Œæ™‚é–“ï¼š{time() - start_time:.2f} ç§’\n")

if __name__ == "__main__":
    main()
