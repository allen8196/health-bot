import os
from time import time

from dotenv import load_dotenv
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_ollama import OllamaLLM
from pymilvus import Collection, connections

from embedding import to_vector

# è¼‰å…¥ .env
load_dotenv()

# ä¸­æ–‡æ‘˜è¦ prompt
SUMMARY_PROMPT = """è«‹å°‡ä»¥ä¸‹ä½¿ç”¨è€…èˆ‡åŠ©ç†çš„å°è©±æ‘˜è¦ç‚ºä¸€æ®µç°¡æ½”æè¿°ï¼Œæ‘˜è¦è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

{summary}
å°è©±å…§å®¹ï¼š
{new_lines}
æ‘˜è¦ï¼š"""

# System äººè¨­
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½è²¼å¿ƒçš„å­«å­/å­«å¥³ï¼Œæ­£åœ¨ç”¨è‡ªç„¶ã€é—œæ‡·çš„èªæ°£å’Œçˆºçˆºå¥¶å¥¶å°è©±ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£è¦ªåˆ‡ç°¡çŸ­ã€‚ä¸è¦è¬›æ•…äº‹æˆ–éé•·çš„å»ºè­°ï¼Œè«‹åƒæ—¥å¸¸å°è©±ä¸€æ¨£ç°¡å–®å›æ‡‰ã€‚"


class HealthChatAgent:
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.llm = self._init_llm()
        self.memory = self._init_memory()

    def _init_llm(self):
        return OllamaLLM(model="adsfaaron/taide-lx-7b-chat:q5")

    def _init_memory(self):
        return ConversationSummaryMemory(
            llm=self.llm,
            memory_key="chat_history",
            prompt=ChatPromptTemplate.from_template(SUMMARY_PROMPT),
        )

    def search_milvus(self, user_text):
        try:
            connections.connect(alias="default", uri="http://localhost:19530")
            collection = Collection("demo1")
            collection.load()
            user_vec = to_vector(user_text)
            results = collection.search(
                data=[user_vec],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=3,
                output_fields=["text"],
            )
            connections.disconnect(alias="default")

            threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.85"))
            relevant_chunks = []

            print("\nğŸ” å‰ 3 ç­†ç›¸ä¼¼æª¢ç´¢çµæœï¼ˆå«ç›¸ä¼¼åº¦ï¼‰")
            for i, hit in enumerate(results[0]):
                score = hit.score
                chunk_text = hit.entity.get("text")
                print(f"Top {i+1} | ç›¸ä¼¼åº¦: {score:.4f}\nå…§å®¹: {chunk_text}\n")
                if score >= threshold:
                    relevant_chunks.append(chunk_text)

            return relevant_chunks
        except Exception as e:
            print(f"[Milvus éŒ¯èª¤] {e}")
            return []

    def chat(self, user_input):
        chunks = self.search_milvus(user_input)
        history = self.memory.load_memory_variables({})["chat_history"]

        context = "\n\n".join(chunks) if chunks else ""

        chat_messages = [SystemMessage(content=SYSTEM_PROMPT)]
        if context:
            chat_messages.append(
                HumanMessage(content=f"ä»¥ä¸‹æ˜¯ä½ å¯ä»¥åƒè€ƒçš„å¥åº·è³‡æ–™ï¼š\n{context}")
            )
        if history:
            chat_messages.append(
                HumanMessage(content=f"éå»çš„å°è©±æ‘˜è¦å¦‚ä¸‹ï¼š\n{history}")
            )

        chat_messages.append(HumanMessage(content=user_input))

        response = self.llm.invoke(chat_messages)
        self.memory.save_context({"input": user_input}, {"output": response})

        print("\nğŸ§  ä½¿ç”¨è€…æ‘˜è¦è¨˜æ†¶ï¼š")
        print(self.memory.load_memory_variables({})["chat_history"])

        return response


def main():
    print("ğŸ‘¤ å¤šä½¿ç”¨è€…å¥åº·å°è©±æ¸¬è©¦æ¨¡å¼")
    user_id = input("è«‹è¼¸å…¥æ¸¬è©¦ç”¨çš„ user_idï¼š").strip()
    agent = HealthChatAgent(user_id)

    print("\nâœ… å°è©±å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' çµæŸã€‚\n")
    while True:
        user_text = input("ğŸ§“ é•·è¼©ï¼š")
        if user_text.lower() in ["exit", "quit"]:
            break
        start_time = time()
        reply = agent.chat(user_text)
        print("ğŸ‘§ é‡‘å­«ï¼š", reply)
        print(f"â±ï¸ åŸ·è¡Œæ™‚é–“ï¼š{time() - start_time:.2f} ç§’\n")


if __name__ == "__main__":
    main()
